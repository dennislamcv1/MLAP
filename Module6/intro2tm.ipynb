{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Text Classification II\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we explore more advanced machine learning techniques with text data. First, we further explore the text classification we did in the previous lesson to see how we can improve. Next, we introduce the concept of n-grams, which are combinations of one or more tokens. For example, bigrams are combinations of two tokens, while trigrams are combinations of three tokens. Finally, we introdce sentiment analysis with a new text data set.\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "[Explore Classifiers](#Explore-Classifiers)\n",
    "\n",
    "[N-Grams](#N-Grams)\n",
    "\n",
    "[N-Gram Classification](#N-Gram-Classification)\n",
    "\n",
    "[Sentiment Analysis](#Sentiment-Analysis)\n",
    "\n",
    "[Stemming](#Stemming)\n",
    "\n",
    "-----\n",
    "\n",
    "Before proceeding with the rest of this notebook, we first include the notebook setup code and we define our _home_ directory.\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up Notebook\n",
    "%matplotlib inline\n",
    "\n",
    "# Standard imports\n",
    "import numpy as np\n",
    "\n",
    "# We do this to ignore several specific warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore Classifiers\n",
    "\n",
    "Let's explore the classifiers we used in the previous lesson. We can get top features(words, tokens) used by a classifier to predict each class. By examining the top words of each class, we get better understanding of the data set and the classifier. We hope this will help us improve the classification.\n",
    "\n",
    "We first repeat the text classification on the twenty newsgroup dataset with MultinomialNB classifier. In the next Code cell, we load the data and create DTM with `TfidfVectorizer`, then train the `MultinomialNB` model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NB (TF-IDF with Stop Words) prediction accuracy =  81.7%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "# To learn more about these data, either browse the relevant \n",
    "# scikit-learn documentation, or enter help(fetch_20newsgroups) in a Code cell\n",
    "\n",
    "# Get training text set\n",
    "train = fetch_20newsgroups(data_home='.', subset='train')\n",
    "# Get testing text set\n",
    "test = fetch_20newsgroups(data_home='.', subset='test')\n",
    "\n",
    "# Create DTM\n",
    "tf_cv = TfidfVectorizer(stop_words='english')\n",
    "train_dtm_tf = tf_cv.fit_transform(train['data'])\n",
    "test_dtm_tf = tf_cv.transform(test['data'])\n",
    "\n",
    "nb = MultinomialNB()\n",
    "nb = nb.fit(train_dtm_tf, train['target'])\n",
    "\n",
    "predicted = nb.predict(test_dtm_tf)\n",
    "scr = 100.0 * nb.score(test_dtm_tf, test['target'])\n",
    "print(f'NB (TF-IDF with Stop Words) prediction accuracy = {scr:5.1f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "We can use the `coef_` attribute of a linear classifier to identify the top features (words, tokens) for each category. (A linear classifier makes a classification decision based on the value of a linear combination of the characteristics or features. i.e., MultinomialNB, LinearSVC, LogisticRegressoin). `coef_` holds coefficients of all the features for each class. Since in text analysis, the features are tokens (words) in the whole training text set. A larger coefficient means more impact of the feature on predicting the class.\n",
    "\n",
    "In the following Code cell we use `coef_` to find out top 5 features (words, tokens) used to predicting each class. I'll explain the code a bit:\n",
    "\n",
    "- First get all features from the vectorizer and convert the return value to a numpy array.\n",
    "- Iterate through all classes, in our case, the 20 newsgroups.\n",
    "- For each class, sort the classifiers' `coef_` attribute. For example, `nb.coef_[0]` is a list of coefficients for the first class, or _alt.atheism_ in our case. `np.argsort` is a numpy sort function that sorts a numpy array, but return a numpy array with indexes of the sorted array. For example, `np.argsort(np.array([2, 1, 3]))` returns a numpy array `[1, 0, 2]`. Because in the original array, the item with index 1 is the smallest, the item with index 0 is the second smallest and the item with index 2 is the largest.\n",
    "- Once we get an index of sorted coefficients, keep the last 5 indexes as top_word_index since those indexes are corresponding to the top 5 largest coefficients.\n",
    "- Use the top_word_index which is an array of indexes to find the top feature names(words).\n",
    "- Reverse the list of words so that the most important one is at the first.\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "alt.atheism:\n",
      "['keith', 'edu', 'god', 'caltech', 'atheists']\n",
      "\n",
      "comp.graphics:\n",
      "['graphics', 'edu', 'image', '3d', 'files']\n",
      "\n",
      "comp.os.ms-windows.misc:\n",
      "['windows', 'edu', 'file', 'dos', 'files']\n",
      "\n",
      "comp.sys.ibm.pc.hardware:\n",
      "['scsi', 'drive', 'ide', 'card', 'edu']\n",
      "\n",
      "comp.sys.mac.hardware:\n",
      "['mac', 'apple', 'edu', 'drive', 'quadra']\n",
      "\n",
      "comp.windows.x:\n",
      "['window', 'motif', 'mit', 'server', 'com']\n",
      "\n",
      "misc.forsale:\n",
      "['sale', 'edu', '00', 'offer', 'shipping']\n",
      "\n",
      "rec.autos:\n",
      "['car', 'com', 'cars', 'edu', 'engine']\n",
      "\n",
      "rec.motorcycles:\n",
      "['bike', 'com', 'dod', 'edu', 'ride']\n",
      "\n",
      "rec.sport.baseball:\n",
      "['edu', 'baseball', 'year', 'team', 'game']\n",
      "\n",
      "rec.sport.hockey:\n",
      "['hockey', 'team', 'game', 'ca', 'edu']\n",
      "\n",
      "sci.crypt:\n",
      "['key', 'clipper', 'encryption', 'chip', 'com']\n",
      "\n",
      "sci.electronics:\n",
      "['edu', 'com', 'use', 'lines', 'subject']\n",
      "\n",
      "sci.med:\n",
      "['pitt', 'edu', 'geb', 'banks', 'gordon']\n",
      "\n",
      "sci.space:\n",
      "['space', 'nasa', 'edu', 'henry', 'moon']\n",
      "\n",
      "soc.religion.christian:\n",
      "['god', 'jesus', 'christians', 'church', 'edu']\n",
      "\n",
      "talk.politics.guns:\n",
      "['gun', 'edu', 'guns', 'com', 'people']\n",
      "\n",
      "talk.politics.mideast:\n",
      "['israel', 'israeli', 'jews', 'turkish', 'armenian']\n",
      "\n",
      "talk.politics.misc:\n",
      "['edu', 'com', 'cramer', 'optilink', 'people']\n",
      "\n",
      "talk.religion.misc:\n",
      "['god', 'sandvik', 'jesus', 'edu', 'com']\n"
     ]
    }
   ],
   "source": [
    "# Display top 5 important words\n",
    "all_words = np.array(tf_cv.get_feature_names())\n",
    "\n",
    "for idx, target in enumerate(train['target_names']):\n",
    "    top_word_index = np.argsort(nb.coef_[idx])[-5:]\n",
    "    tn_lst = [word for word in all_words[top_word_index]]\n",
    "    tn_lst.reverse()\n",
    "\n",
    "    print(f'\\n{target}:')\n",
    "    print(tn_lst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "From the top words that are used to identify each class(label), we can see that \"edu\" and \"com\" are in the top 5 words of many classes. Those words are from email addresses which every message has. They don't really have much predicting power. The fact that they are in top 5 of so many classes proves this. In the previous lesson we mentioned that we hope that TF-IDF can help mitigate the problem but seems it doesn't. We can solve this problem by defining our own stop words and add \"com\" and \"edu\" to the stop words to filter them out manually. We'll also add \"re\" which is in many message headers to the stop words. We demonstrate this approach in the following Code cell.\n",
    "\n",
    "The change doesn't improve the classification accuracy. This shows that not all improvements work as desired. But with the customized stop words, we do get more meaningful top words. This can be very valuable information sometimes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/stopwords\u001b[0m\n\n  Searched in:\n    - '/home/jovyan/nltk_data'\n    - '/opt/conda/nltk_data'\n    - '/opt/conda/share/nltk_data'\n    - '/opt/conda/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     85\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m                     \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'{}/{}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubdir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzip_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/nltk/data.py\u001b[0m in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    700\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'\\n%s\\n%s\\n%s\\n'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 701\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    702\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/stopwords.zip/stopwords/\u001b[0m\n\n  Searched in:\n    - '/home/jovyan/nltk_data'\n    - '/opt/conda/nltk_data'\n    - '/opt/conda/share/nltk_data'\n    - '/opt/conda/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-6a62be2a6201>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#get current stop words\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mstop_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'english'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#add com, edu and re to stop words\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, attr)\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"LazyCorpusLoader object has no attribute '__bases__'\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    124\u001b[0m         \u001b[0;31m# This looks circular, but its not, since __load() changes our\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m         \u001b[0;31m# __class__ to something new:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     86\u001b[0m                     \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'{}/{}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubdir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzip_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0;31m# Load the corpus.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m                 \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'{}/{}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubdir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/nltk/data.py\u001b[0m in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    699\u001b[0m     \u001b[0msep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'*'\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m70\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    700\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'\\n%s\\n%s\\n%s\\n'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 701\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    702\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    703\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/stopwords\u001b[0m\n\n  Searched in:\n    - '/home/jovyan/nltk_data'\n    - '/opt/conda/nltk_data'\n    - '/opt/conda/share/nltk_data'\n    - '/opt/conda/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "#get current stop words\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "#add com, edu and re to stop words\n",
    "stop_words.extend(['com', 'edu', 're'])\n",
    "\n",
    "# Create DTM, use custom defined stop words\n",
    "tf_cv = TfidfVectorizer(stop_words=stop_words)\n",
    "train_dtm_tf = tf_cv.fit_transform(train['data'])\n",
    "test_dtm_tf = tf_cv.transform(test['data'])\n",
    "\n",
    "nb = MultinomialNB()\n",
    "nb = nb.fit(train_dtm_tf, train['target'])\n",
    "\n",
    "predicted = nb.predict(test_dtm_tf)\n",
    "scr = 100.0 * nb.score(test_dtm_tf, test['target'])\n",
    "print(f'NB (TF-IDF with Stop Words) prediction accuracy = {scr:5.1f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display top 5 important words\n",
    "all_words = np.array(tf_cv.get_feature_names())\n",
    "\n",
    "for idx, target in enumerate(train['target_names']):\n",
    "    top_word_index = np.argsort(nb.coef_[idx])[-5:]\n",
    "    tn_lst = [word for word in all_words[top_word_index]]\n",
    "    tn_lst.reverse()\n",
    "\n",
    "    print(f'\\n{target}:')\n",
    "    print(tn_lst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Next, we apply LogisticRegression on the DTM which is created with customized stop words, but the classification accuracy doesn't improve again. Then we print out the top 5 words from each class. We can see that `LogisticRegerssion` gives different top words to that of `MultinomialNB`. This is because the different models use different features for prediction. There are still quite a few overlaps of words in the top words of each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "lr = LogisticRegression(C=1000)\n",
    "\n",
    "lr = lr.fit(train_dtm_tf, train['target'])\n",
    "predicted = lr.predict(test_dtm_tf)\n",
    "\n",
    "scr = 100.0 * lr.score(test_dtm_tf, test['target'])\n",
    "print(f'LR (TF-IDF with Stop Words) prediction accuracy = {scr:5.1f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, target in enumerate(train['target_names']):\n",
    "    top_word_index = np.argsort(lr.coef_[idx])[-5:]\n",
    "    tn_lst = [word for word in all_words[top_word_index]]\n",
    "    tn_lst.reverse()\n",
    "\n",
    "    print(f'\\n{target}:')\n",
    "    print(tn_lst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "<font color='red' size = '5'> Student Exercise </font>\n",
    "\n",
    "In the preceding cells, we print out top words used in predicting each class for MultinomialNB and LogisticRegression. In the following Code cell, try to display the top 5 words used by LinearSVC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "[[Back to TOC]](#Table-of-Contents)\n",
    "\n",
    "## N-Grams\n",
    "\n",
    "Formally, a [_n-gram_][ngd] is a contiguous sequence of **n** items from a parent sequence of items, such as characters or words in a text document. In general, we will focus solely on words in a document. Thus, our initial approach has simply been to look at unigrams or single words in a document when building a classification model. However, sometimes the combination of words can be more descriptive, for example, _unbelievably bad_ is generally viewed as a more powerful description than just _bad_. As a result, the concept of an _n-gram_ was created, where collections of words can be treated as features. In fact Google allows a user to search for [specific n-gram][gnv] combinations in books that they have scanned.\n",
    "\n",
    "While this clearly can improve classification power, it also increases computational requirements. This is a result of the rise in the number of possible features. While this is not a problem for small vocabularies, for larger vocabularies (and corresponding documents) the number of possible features can quickly become very large. Thus, many text mining applications will make use of Hadoop or Spark clusters to leverage the inherent parallelism in these tasks.\n",
    "\n",
    "To demonstrate using n-grams, we first demonstrate the concept on a single sentence.\n",
    "\n",
    "-----\n",
    "[gnv]: https://books.google.com/ngrams\n",
    "[ngd]: https://en.wikipedia.org/wiki/N-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_text = 'This course introduces many concepts in data science.'\n",
    "\n",
    "# Tokenize sentance\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv = CountVectorizer(ngram_range=(1,3))\n",
    "\n",
    "# Analyze sentance\n",
    "tk_func = cv.build_analyzer()\n",
    "\n",
    "# Display n-grams\n",
    "print(tk_func(my_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "`my_text` has 8 words. When we set `ngram_range=(1,3)` which means create tokens for 1, 2 and 3 consecutive words, there are 21 tokens now.\n",
    "\n",
    "We can create a new document-term matrix based on this sentence, then use this matrix (or vector since it is only one row), to provide a representation space for new sentences. In the following Code cell, we tokenize our original sentence, and sort the resulting vocabulary (i.e., n-grams) with their ranking (or order). Next, we create a second, simple sentence and compute the indices into the original DTM for the n-grams in the new sentence. The result is displayed as a bit vector, where `1` means the corresponding n-gram is in the new sentence, and `0` means it is not.\n",
    "\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize sentence\n",
    "cv = cv.fit([my_text])\n",
    "\n",
    "# Sort tokens\n",
    "import operator\n",
    "my_voc = sorted(cv.vocabulary_.items(), key=operator.itemgetter(1))\n",
    "\n",
    "# Display token mapping\n",
    "print('Token mapping:')\n",
    "print(40*'-')\n",
    "\n",
    "for tokens, rank in my_voc:\n",
    "    print(rank, tokens)\n",
    "\n",
    "# Display new sentence\n",
    "print(40*'-')\n",
    "out_list = ['This course is data science!']\n",
    "\n",
    "# Transform new sentence to original sentence DTM\n",
    "xsm = cv.transform(out_list)\n",
    "print(out_list)\n",
    "\n",
    "# Display count vector indices for new sentance tokens\n",
    "print(40*'-')\n",
    "print(xsm.todense())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "<font color='red' size = '5'> Student Exercise </font>\n",
    "\n",
    "In the preceding cells, we used `CountVectorizer` to create n-gram tokens. Now that you have run the notebook, go back and make the following changes to see how the results change.\n",
    "\n",
    "1. Change the `CountVectorizer` to use stop words. How does this change the tokens and mappings? \n",
    "2. Try changing the ngram range to different values (e.g., `ngram_range=(1,4)`). Notice how the number of tokens quickly increase."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "[[Back to TOC]](#Table-of-Contents)\n",
    "\n",
    "## N-Gram Classification\n",
    "\n",
    "Having n-grams often offers improved classification, since word or token combinations often include more information than single words or tokens. For example, _University Illinois_ means more than just _University_ and _Illinois_. We can build on our previous simple text classification to now develop a more complete code example that builds a feature vector containing both single words and n-grams from the documents. We use this new sparse matrix to classify the documents by using our simple Naive Bayes classifier. However, we actually get a slightly worse accuracy score. This shows again that not all the optimization attempts will improve the model performance.\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DTM\n",
    "tf_cv = TfidfVectorizer(stop_words='english', ngram_range=(1,2))\n",
    "train_dtm_tf = tf_cv.fit_transform(train['data'])\n",
    "test_dtm_tf = tf_cv.transform(test['data'])\n",
    "\n",
    "nb = MultinomialNB()\n",
    "nb = nb.fit(train_dtm_tf, train['target'])\n",
    "\n",
    "predicted = nb.predict(test_dtm_tf)\n",
    "scr = 100.0 * nb.score(test_dtm_tf, test['target'])\n",
    "print(f'NB (TF-IDF with ngram_range 1-2) prediction accuracy = {scr:5.1f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "<font color='red' size = '5'> Student Exercise </font>\n",
    "\n",
    "In the preceding cells, we set ngram_range to (1,2) and apply MultinomialNB. In the next Code cell, try applying other classifiers like LinearSRC or LogisticRegression with ngram_range (1,2) to see how the results change.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "[[Back to TOC]](#Table-of-Contents)\n",
    "\n",
    "\n",
    "## Sentiment Analysis\n",
    "\n",
    "Sentiment Analysis aims to identify and extract emotion or opinion from text. It is a useful technology that businesses can apply in social media, customer reviews, and customer support.\n",
    "\n",
    "In this section, we turn to a classification problem where the goal is to classify based on sentiment, negative or positive. We will approach this as a simple classification problem. \n",
    "\n",
    "We first load our data. We will use the movie reviews data built-in in the `nltk.corpus` module. The movie_reviews corpus contains 2000 movie reviews with sentiment polarity classification. Out of the 2000 reviews, the first 1000 are negative reviews, and next 1000 are positive reviews.\n",
    "\n",
    "In the following Code cell, we first load the movie reviews corpus from ntlk. Each review has a sentiment label, either _'neg'_ or _'pos'_ (stands for negative and positive respectively). The reviews are stored as a list of words. We combine the list of words to generate the text reviews, which will be our text data. We then create classification labels by mapping _'neg'_ to 0 and _'pos'_ to 1. Finally, we split the data set to training and testing date set. We print out the first review and its label in the training text set to show what a review looks like.\n",
    "\n",
    "**Note:** Don't worry if you have trouble understanding the code to load the movie reviews from nltk. Just focus on the final text data set.\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.corpus import movie_reviews\n",
    "#load movie reviews, each review is a list of words\n",
    "documents = [(list(movie_reviews.words(fileid)), category)\n",
    "             for category in movie_reviews.categories()\n",
    "             for fileid in movie_reviews.fileids(category)]\n",
    "\n",
    "print (\"Number of Reviews:\", len(documents))\n",
    "#shuffle reviews to mix negative and positive reviews\n",
    "#set random seed for reproducibility\n",
    "random.seed(23)\n",
    "random.shuffle(documents)\n",
    "\n",
    "#list to store all review text\n",
    "text_data = []\n",
    "#label\n",
    "label = []\n",
    "for i in range(len(documents)):\n",
    "    #join list of words to create a review and add to text_data\n",
    "    text_data.append(' '.join(documents[i][0]))\n",
    "    #map neg to 0, pos to 1 and add to label\n",
    "    label.append(0 if documents[i][1]=='neg' else 1)\n",
    "\n",
    "print(\"Number of Negative Reviews:\", label.count(0))\n",
    "print(\"Number of Positive Reviews:\", label.count(1))    \n",
    "\n",
    "#split to train and text\n",
    "mvr_train, mvr_test, y_train, y_test = train_test_split(text_data, label, test_size=0.25, random_state=23)\n",
    "\n",
    "#print one example review in the training text set\n",
    "print(\"Sample Review:\")\n",
    "print('Label:', y_train[0])\n",
    "print(mvr_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Next we create bag of words from training text set with `TfidfVectorizer`, transform both training and testing set to the document term matrix, then apply `MultinomaiNB` classifier. We print out the score, the classification report and plot the confusion matrix.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Naive Bayes pipeline to classify\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import metrics\n",
    "\n",
    "tf_cv = TfidfVectorizer(stop_words='english')\n",
    "train_dtm_tf = tf_cv.fit_transform(mvr_train)\n",
    "test_dtm_tf = tf_cv.transform(mvr_test)\n",
    "\n",
    "# Fit model, predict, and display results\n",
    "nb = MultinomialNB()\n",
    "nb = nb.fit(train_dtm_tf, y_train)\n",
    "y_pred = nb.predict(test_dtm_tf)\n",
    "scr = 100.0 * nb.score(test_dtm_tf, y_test)\n",
    "print(f'NB (TF-IDF with stop words) prediction accuracy = {scr:5.1f}%')\n",
    "print(metrics.classification_report(y_test, y_pred, target_names = ['Negative', 'Positive']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helper_code import mlplots as ml\n",
    "\n",
    "#fig, ax = plt.subplots(figsize=(13, 10))\n",
    "# Call confusion matrix plotting routine\n",
    "ml.confusion(y_test, y_pred, ['Negative', 'Positive'], 'Naive Bayes Model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Next we display top words in predicting positive reviews. Note that since there are only two classes, either 0 or 1, there's only one list of coefficients for the positive class. Because the model only needs to classify positive reviews and the rest are negative reviews. If you want to find out top words for predicting negative reviews, you may reverse the label, map _neg_ to 1 and *pos* to 0 and repeat following code.\n",
    "\n",
    "We print out the top 20 words for predicting positive movie reviews. There are good, great, best and love which make sense, but there are also many words like film, movie, story, and really, etc. that don't make much sense. We can try to add words like film and movie to the stop words since those words are likely to appear in both positive and negative reviews. Or we can try another algorithm to see if we get better result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words = np.array(tf_cv.get_feature_names())\n",
    "\n",
    "top_word_index = np.argsort(nb.coef_[0])[-20:]\n",
    "tn_lst = [word for word in all_words[top_word_index]]\n",
    "tn_lst.reverse()\n",
    "\n",
    "print(f'\\nTop 20 Words of Positive Reviews:')\n",
    "print(tn_lst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "We now try `LogisticRegression` with `C` set to `1000`. This model gives better accuracy and a much better list of top words for predicting positive reviews.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression(C=1000)\n",
    "\n",
    "lr = lr.fit(train_dtm_tf, y_train)\n",
    "predicted = lr.predict(test_dtm_tf)\n",
    "\n",
    "scr = 100.0 * lr.score(test_dtm_tf, y_test)\n",
    "print(f'LR (TF-IDF with Stop Words) prediction accuracy = {scr:5.1f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_word_index = np.argsort(lr.coef_[0])[-20:]\n",
    "tn_lst = [word for word in all_words[top_word_index]]\n",
    "tn_lst.reverse()\n",
    "\n",
    "print(f'\\nTop 20 Words of Positive Reviews:')\n",
    "print(tn_lst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Next we reverse the label value, train the model again and get the top words for predicting negative reviews. Most words in the list make a lot of sense.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reverse label value so that negative reviews have label 1\n",
    "y_train_reverse = [0 if y==1 else 1 for y in y_train]\n",
    "lr = lr.fit(train_dtm_tf, y_train_reverse)\n",
    "\n",
    "top_word_index = np.argsort(lr.coef_[0])[-20:]\n",
    "tn_lst = [word for word in all_words[top_word_index]]\n",
    "tn_lst.reverse()\n",
    "\n",
    "print(f'\\nTop 20 words of Negative Reviews:')\n",
    "print(tn_lst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "<font color='red' size = '5'> Student Exercise </font>\n",
    "\n",
    "In the previous Code cells, we classified movie reviews with default English stop words. Try adding common words in movie reviews like movie and film to the stop words. What impact does this change have on each model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "[[Back to TOC]](#Table-of-Contents)\n",
    "\n",
    "## Stemming\n",
    "\n",
    "In the previous lesson, we introduced the concept of stemming. In the next Code cell, we demonstrate how to apply stemming in text classification. We will use `PorterStemmer` in the `nltk` module for stemming.\n",
    "\n",
    "We first define a function `tokenize`. The function has one argument `text` which is the text to be tokenized. The function uses `nltk.word_tokenize` function to tokenize `text` then apply `PorterStemmer` to stem the tokens. We then  set `tokenizer` argument in `CounterVectorizer` or `TfidfVectorizer` with this custom `tokenize` function and use the new vectorizer to create bag of words.\n",
    "\n",
    "The following code takes longer to finish due to stemming, but it does give a better classification accuracy.\n",
    "\n",
    "-----\n",
    "[ws]: https://en.wikipedia.org/wiki/Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import nltk\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "# Define function to tokenize text and apply stemmer\n",
    "def tokenize(text):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    tokens = [token for token in tokens if token not in string.punctuation]\n",
    "\n",
    "    stemmer = PorterStemmer()\n",
    "    stems = map(stemmer.stem, tokens)\n",
    "    return stems\n",
    "\n",
    "# use custom tokenize when creating vectorizer\n",
    "tf_cv = TfidfVectorizer(tokenizer=tokenize)\n",
    "train_dtm_tf = tf_cv.fit_transform(mvr_train)\n",
    "test_dtm_tf = tf_cv.transform(mvr_test)\n",
    "\n",
    "lr = LogisticRegression(C=1000)\n",
    "\n",
    "lr = lr.fit(train_dtm_tf, y_train)\n",
    "predicted = lr.predict(test_dtm_tf)\n",
    "\n",
    "scr = 100.0 * lr.score(test_dtm_tf, y_test)\n",
    "print(f'LR (TF-IDF with Stemming) prediction accuracy = {scr:5.1f}%')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ancillary Information\n",
    "\n",
    "The following links are to additional documentation that you might find helpful in learning this material. Reading these web-accessible documents is completely optional.\n",
    "\n",
    "1. Wikipedia articles on [n-grams][wng], [Stemming][wst], and [Lemmatization][wl]\n",
    "1. Google [n-gram viewer][gnv]\n",
    "1. Alternative [n-gram viewer][anv]\n",
    "1. Blog on Sentiment Analysis with NLTK, [Part III][bsa3] and [Part IV][bsa4]\n",
    "1. An online [Stemming Demo][std] using NLTK\n",
    "1. A [treatise on Snowball][tsb] discussing, in depth, the process of stemming.\n",
    "\n",
    "-----\n",
    "\n",
    "[wst]: https://en.wikipedia.org/wiki/Stemming\n",
    "[wl]: https://en.wikipedia.org/wiki/Lemmatisation\n",
    "[wtc]: https://en.wikipedia.org/wiki/Document_clustering\n",
    "\n",
    "[tsb]: http://snowball.tartarus.org/texts/introduction.html\n",
    "[std]: http://text-processing.com/demo/stem/\n",
    "\n",
    "[wng]: https://en.wikipedia.org/wiki/N-gram\n",
    "\n",
    "[gnv]: https://books.google.com/ngrams\n",
    "[anv]: http://xkcd.culturomics.org\n",
    "\n",
    "[bsa3]: http://streamhacker.com/2010/05/24/text-classification-sentiment-analysis-stopwords-collocations/\n",
    "[bsa4]: http://streamhacker.com/2010/05/24/text-classification-sentiment-analysis-stopwords-collocations/\n",
    "\n",
    "[msdr]: http://research.microsoft.com/pubs/150728/FnT_dimensionReduction.pdf\n",
    "[lle]: http://science.sciencemag.org/content/290/5500/2323.abstract\n",
    "[ica]: http://www.cs.rutgers.edu/~mlittman/topics/dimred02/kolenda99independent.pdf1. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "**&copy; 2019: Gies College of Business at the University of Illinois.**\n",
    "\n",
    "This notebook is released under the [Creative Commons license CC BY-NC-SA 4.0][ll]. Any reproduction, adaptation, distribution, dissemination or making available of this notebook for commercial use is not allowed unless authorized in writing by the copyright holder.\n",
    "\n",
    "[ll]: https://creativecommons.org/licenses/by-nc-sa/4.0/legalcode"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
